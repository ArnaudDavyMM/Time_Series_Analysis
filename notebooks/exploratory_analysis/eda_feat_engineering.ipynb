{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ef5700-bd71-4e44-99c4-6f17485d94f5",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "This project is structured into **four major components**, each addressing a critical phase of **time series forecasting** using the **Rossmann Store Sales dataset**:\n",
    "\n",
    "üîπ **Part 1**: Data Ingestion, Exploratory Data Analysis (EDA)\n",
    "\n",
    "We begin by importing the dataset, performing a thorough exploratory data analysis to assess data quality, uncover patterns, and identify potential issues such as missing values, anomalies, and inconsistencies.\n",
    "\n",
    "üîπ **Part 2**: Feature Engineering and Data Visualization\n",
    "\n",
    "This section delves into engineering relevant features (predictors) to prepare the data for modeling and presents key visualizations to uncover underlying trends, recurring patterns, and seasonal effects.\n",
    "\n",
    "üîπ **Part 3**: Classical Time Series Analysis and Forecasting\n",
    "\n",
    "This section focuses on widely adopted forecasting techniques in the data science domain. We will implement and evaluate several standard algorithms, including:\n",
    "\n",
    " - **Statistical Models**: ARIMA, SARIMA\n",
    "    \n",
    " - **Ensemble Methods:** XGBoost, LightGBM\n",
    "    \n",
    " - **Facebook Prophet:** A robust model for time series forecasting with built-in seasonality and holiday effects\n",
    "    \n",
    " - **Deep Learning Models:** LSTM, Temporal Fusion Transformers (TFT), N-BEATS\n",
    "\n",
    "üîπ **Part 4**: Hybrid Time Series Forecasting\n",
    "\n",
    "This advanced section explores hybrid modeling approaches typically used by experienced data scientists. These models combine the strengths of multiple algorithms to improve forecasting accuracy:\n",
    "\n",
    " - **ARIMA + XGBoost**\n",
    "    \n",
    " - **Prophet + LightGBM / XGBoost**\n",
    "    \n",
    " - **Prophet + LSTM**\n",
    "    \n",
    " - **TFT + ARIMA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12025e3-180d-4431-99fd-8bccc2a93bab",
   "metadata": {},
   "source": [
    " # Model Selection Strategy\n",
    "----------------------------\n",
    "\n",
    "The choice of forecasting algorithm depends on the characteristics of the dataset and the domain expertise of the practitioner. In this project, we will experiment with all the above methods and compare their performance to determine the most effective approach for our data.\n",
    "\n",
    "**Let‚Äôs dive in and explore which model delivers the most accurate forecasts!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b3337-c185-4bb0-adad-9ec9f641d3e6",
   "metadata": {},
   "source": [
    "## Part 1: Data Ingestion and Exploratory Data Analysis (EDA)\n",
    "-------------------------------------------------------------\n",
    "\n",
    "In this section, we will focus on **ingesting** the Rossmann Store Sales dataset and conducting a comprehensive **Exploratory Data Analysis (EDA)** prior to applying time series modeling and forecasting techniques. The workflow will proceed through the following steps:\n",
    "\n",
    "1. **Import Libraries and Dependencies:** Load all necessary Python libraries and packages required for data manipulation, visualization, and analysis.\n",
    "\n",
    "1. **Data Ingestion:** Load the Rossmann Store Sales dataset into the working environment for analysis.\n",
    "\n",
    "3. **Exploratory Data Analysis (EDA):** Perform a detailed examination of the dataset to understand its structure and key characteristics:\n",
    "\n",
    "   - **Inspect dataset metadata**: data types, number of observations (rows), and variables (columns)\n",
    "\n",
    "    - **Identify and quantify missing values**\n",
    "\n",
    "    - **Detect and handle duplicate records**\n",
    "\n",
    "    - **Generate summary statistics** (mean, median, standard deviation, etc.)\n",
    "\n",
    "    - **Analyze individual features and their distributions**\n",
    "\n",
    "    - **Apply feature engineering techniques to enhance model readiness**\n",
    "\n",
    "    - **Evaluate feature correlations to identify relationships**\n",
    "\n",
    "    - **Visualize data using appropriate plots and charts**\n",
    "\n",
    "    - **Conduct deeper analysis to uncover trends, patterns, and seasonality within the time series**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd832622-b6c7-4b7f-a4ef-26eeea11e6a1",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports Libraries\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a4ece7-9e78-4878-b52b-28605ad85ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563f70e-b121-4e17-8522-e90be7726f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Ingestion\n",
    "print(\"Step 1: Setup and Import Libraries in progress...\")\n",
    "time.sleep(1)  # Simulate processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d6c5fa-aee0-4e1f-9e59-840fdba88e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation & Processing\n",
    "import os\n",
    "import holidays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import scipy.stats as stats\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format','{:.2f}'.format)\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Rossman Store Sales Time Series Analysis - Part 1\")\n",
    "print(\"=\"*60)\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28356173-747b-4d1f-8398-e5ccca45e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Setup and Import Liraries completed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce5b68-7d10-4d23-aa9f-34b449a511ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start analysis\n",
    "\n",
    "analysis_begin = pd.Timestamp.now()\n",
    "\n",
    "bold_start = '\\033[1m'\n",
    "bold_end = '\\033[0m'\n",
    "\n",
    "print(\"üîç Analysis Started\")\n",
    "print(f\"üü¢ Begin Date: {bold_start}{analysis_begin.strftime('%Y-%m-%d %H:%M:%S')}{bold_end}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca539c-3bf4-4d74-bc5a-0b4a77793146",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data Ingestion\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d6ee5-e88f-43e5-89fd-6e454bfb56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Ingestion\n",
    "print(\"Step 2: Data Ingestion in progress...\")\n",
    "time.sleep(1)  # Simulate processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24c581c-c5aa-4b97-ab72-64a64f2e9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use absolute path to avoid pwd issues\n",
    "base_path = Path.home() /\"Desktop\"/\"Time_Series_Analysis\"/\"data\"/\"raw\"\n",
    "train_path = base_path /\"Retail_train_data.csv\"\n",
    "test_path = base_path / \"Retail_test_data.csv\"\n",
    "\n",
    "# Check if files exist\n",
    "print(f\"Looking for files in: {base_path}\\n\")\n",
    "print(f\"Train file exists: {train_path.exists()}\")\n",
    "print(f\"Test file exists: {test_path.exists()}\")\n",
    "\n",
    "if train_path.exists() and test_path.exists():\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    print(\"\\nFiles loaded successfully!\\n\")\n",
    "    print(f\"Train data shape: {train_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\\n\")\n",
    "    print('There are : %s Rows and %s Columns in training data' % (str(train_df.shape[0]) ,str(train_df.shape[1])))\n",
    "    print('There are : %s Rows and %s Columns in testing data' % (str(test_df.shape[0]) ,str(test_df.shape[1])))\n",
    "else:\n",
    "    print(\"Files not found. Please check the file paths and names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1b626-5584-493e-89b2-4b13da6daec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Data Ingestion completed.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988d899-2b50-45c1-82de-8e6e2c196546",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis (EDA)\n",
    "## 3.1. Basic Inspection\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb8f22-7014-4523-8a1b-7a6338f396a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Exploratory Data Analysis (EDA)\n",
    "print(\"üìä Step 3: Exploratory Data Analysis in progress...\")\n",
    "time.sleep(1)  # Simulate processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157b20d-8631-44d4-9512-0f305dbd184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns = train_df.columns.str.lower()\n",
    "test_df.columns = test_df.columns.str.lower()\n",
    "\n",
    "# --- BASIC INFO AND DUPLICATES ---\n",
    "print(\"DataFrame Info:\")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b18d13a-bcf7-4377-b468-a9ff17d584e8",
   "metadata": {},
   "source": [
    "## View or Display Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f2327-6fa3-4c87-8cb8-57cca0d61d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTrain Data Preview:\")\n",
    "print(train_df.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef996b4e-34d3-421d-a3a5-7c04ff033f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['date'].min(), train_df['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5a0bc-0ca6-450f-b03c-79e1d74795c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sort_values('date', inplace =True, ascending =True)\n",
    "\n",
    "print(\"\\nTrain Data Preview:\")\n",
    "print(train_df.head(), \"\\n\")\n",
    "print(\"\\nTest Data Preview:\")\n",
    "print(test_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ffbfb-ae1a-4ef3-ac43-57f4f1aa0897",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df.sort_values('date', inplace =True, ascending =True)\n",
    "\n",
    "print(\"\\nTest Data Preview:\")\n",
    "print(train_df.head(), \"\\n\")\n",
    "print(\"\\nTest Data Preview:\")\n",
    "print(test_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c3b83-bb1c-41e8-95af-6f51d470adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing Values per Column:\")\n",
    "print(train_df.isna().sum())\n",
    "print('\\nNumber of duplicated rows:', train_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73df65c-335a-4a11-9fa5-c5fb83df4be4",
   "metadata": {},
   "source": [
    "## 3.2 Summary Statistics\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e0c87-14b2-4890-a2db-9a6d5b28ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSummary Statistics:\")\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0067e-5ac7-479f-8604-868b77d5da11",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "\n",
    "### This my previous version of Statistical Analysis\n",
    "\n",
    "1. **Store-Level Insights :**\n",
    " *  Store IDs range from **1 to 1115**, suggesting over a thousand **(1115)** unique stores in the dataset.\n",
    " *  The distribution of store IDs **(mean ‚âà 558, std ‚âà 322)** implies a relatively even spread across stores, but some clustering may exist around the median (558).\n",
    "   \n",
    "2. **Temporal Patterns :**\n",
    " *  DayOfWeek ranges from **1 to 7**, with a mean of **~4.00**‚Äîindicating a fairly uniform distribution of entries across all days.\n",
    " *  Could be interesting to examine how sales and customers vary by weekday, especially near the weekend peak **(e.g., Friday/Saturday)**.\n",
    "\n",
    "3. **Sales & Customer Traffic:**\n",
    " *  Average daily sales per entry: **5760.84** with a very wide spread **(std ‚âà 3857.57)**, and a maximum sale day of **$41,551**‚Äîa strong hint at major spikes for certain stores or events.\n",
    " *  Customer count averages **~633** per day, also with high variability **(max nearly 7,400)**, pointing to inconsistent foot traffic across locations and days.\n",
    " *  Zero-sales and zero-customer days are quite high **(‚âà 17% of records)**. These likely correspond to closed stores or unusual business days and should be accounted for in any forecasting or revenue analysis.\n",
    " *   The mean for sales is **5760.843 and the median 5731.00**, suggesting the **sales distribution is right skewed**\n",
    "\n",
    "5. **Store Availability:**\n",
    " *  Open flag mean is **0.829**, meaning stores are open roughly **83%** of the time.\n",
    "\n",
    "6. **Promotional Impact**\n",
    " *  38% of entries have active promotions, and sales/customer variability suggests these promotions could have significant but uneven impact.\n",
    " *  Worth analyzing sales uplift due to **Promo = 1 compared to Promo = 0**.\n",
    "\n",
    "7. **School Holidays**\n",
    " *  **Only 17.2%** of entries fall on a school holiday, suggesting that sales are not high on School Holidays\n",
    " *  This feature may influence customer counts, especially for stores near schools or those that rely on family shoppers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac88e20-31cf-42e1-8ade-5f0b8ffc841b",
   "metadata": {},
   "source": [
    "--------------------------------------------------------\n",
    "### This my current (updated) version of Statistical Analysis\n",
    "\n",
    "Looking at these retail summary statistics, several key insights emerge about this dataset's business patterns and data quality:\n",
    "\n",
    "**Business Operations Pattern**\n",
    "The data reveals a clear weekly rhythm with stores typically open **5-6 days per week (82.9% open rate)**. The day-of-week distribution **(mean ~4.0, std 2.0)** suggests relatively even coverage across the week, though weekends likely see different patterns given the standard deviation.\n",
    "\n",
    "**Sales Performance Distribution**\n",
    "Daily sales **average ‚Ç¨5,761** with substantial variation **(std ‚Ç¨3,858)**, indicating significant differences between high and low-performing periods. The distribution appears right-skewed, as the **median (‚Ç¨5,731)** sits below the **75th percentile (‚Ç¨7,847)**, while the **maximum of ‚Ç¨41,551** represents exceptional peak performance days.\n",
    "\n",
    "**Customer traffic** follows a similar pattern, **averaging 633 customers per day** with the same **right-skewed distribution**. The sales-to-customer ratio of approximately **‚Ç¨9.11 per customer** suggests this could be a grocery or everyday retail environment.\n",
    "\n",
    "**Promotional and Seasonal Effects**\n",
    "Promotions run on **38%** of operating days, suggesting strategic rather than constant promotional activity. School holidays affect only **17.2%** of the dataset, indicating this captures mostly regular school periods with some holiday shopping included.\n",
    "\n",
    "**Data Quality Concerns**\n",
    "The most striking finding is that **168,494 rows (17.1%)** show zero sales, with an almost identical number showing zero customers **(168,492)**. This near-perfect correlation suggests these represent genuine store closures rather than data collection errors. These closed days significantly impact the overall averages and explain why the \"Open\" variable shows **82.9% rather than 100%**.\n",
    "\n",
    "**Strategic Implications**\n",
    "The wide performance range and promotional frequency suggest opportunities for optimizing both timing and targeting of marketing efforts. The substantial day-to-day variation in both sales and customer counts indicates strong external factors **(likely day-of-week effects, promotions, and seasonal patterns)** that could be leveraged for better forecasting and inventory management.\n",
    "\n",
    "This appears to be a comprehensive retail dataset suitable for time series analysis, promotional effectiveness studies, and customer behavior modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45247aa9-f293-427d-adae-d03d3818cb50",
   "metadata": {},
   "source": [
    "## 3.3 Variables Analysis\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994dda02-d381-4b67-9ff9-218d51511fad",
   "metadata": {},
   "source": [
    "#### 1. Suspicious Days (Quality Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f6250-3fe6-4d7f-b64f-30cb60f26e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days with zero customers and non-zero sales (quality check)\n",
    "odd_days = train_df[(train_df[\"customers\"] == 0) & (train_df[\"sales\"] > 0)]\n",
    "print(f\"Suspicious days with sales but no customers: {odd_days.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83017689-aad1-4109-80bc-3a3fa25618f2",
   "metadata": {},
   "source": [
    "#### 2. Unique days of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0d2f1-b11f-453f-9062-b4a6c6e5669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_days = train_df['dayofweek'].sort_values().unique()\n",
    "print(f\"Unique days of the week in the dataset: {unique_days}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e025485-2e4b-46c3-b930-6336926a488a",
   "metadata": {},
   "source": [
    "#### 3. Unique State Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5704f52-6f19-4baa-82f6-c44eca943653",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stateholiday = train_df['stateholiday'].unique()\n",
    "print(f\"Unique State holiday in the dataset: {unique_stateholiday}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f10af-38df-421d-80bd-dca2738927e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- That mix of '0' (string) and 0 (integer) in your array means the column has inconsistent data types ‚Äî which can definitely mess up mappings and analysis.\n",
    "\n",
    "# -- Clean the Data Types\n",
    "train_df['stateholiday'] = train_df['stateholiday'].astype(str)\n",
    "\n",
    "# --- Sorted Output with counts\n",
    "print(\"State Holiday Value Counts:\")\n",
    "print(train_df['stateholiday'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e7454-0f13-4ab8-8ced-8a3184e5fe39",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "### State Holidays namming convention\n",
    "\n",
    " .  **a** stands for **Public Holiday**\n",
    "\n",
    " .  **b** stands for **Easter Holiday**\n",
    "\n",
    " .  **c** stands for **Christmas Holiday**\n",
    "\n",
    " .  **0** or None means **No holiday**\n",
    "\n",
    "These codes were defined by the dataset creators to simplify **holiday categorization across German states**. So when we preprocess the data, we map those codes to their actual meanings to make analysis and visualization more intuitive.\n",
    "\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6a27a-0c32-4ca8-a851-49e24028f13c",
   "metadata": {},
   "source": [
    "#### 4. Closed Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9da22-8b7a-492d-bf70-7b5294d3782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_count = (train_df['open'] == 0).sum()\n",
    "print(f\"\\nTotal number of times stores were closed: {bold_start}{closed_count}{bold_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ef3703-7131-4d6c-899d-28b6014b1afd",
   "metadata": {},
   "source": [
    "#### 5. Opened_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bafac5-9162-48f6-b8dd-6adca19e3b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "opened_count = (train_df['open'] == 1).sum()\n",
    "print(f\"\\nTotal number of times stores were opened: {bold_start}{opened_count}{bold_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f2c7b7-90d6-4356-bbbd-acb1edb345b1",
   "metadata": {},
   "source": [
    "#### 6. Compare Open vs Closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6be60-c7c9-40c7-bdb1-c7b9559d50f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_days = len(train_df)\n",
    "\n",
    "closed_count = (train_df['open'] == 0).sum()\n",
    "opened_count = (train_df['open'] == 1).sum()\n",
    "\n",
    "print(f\"Opened: {bold_start}{opened_count}{bold_end} times ({bold_start}{(opened_count / total_days) * 100:.2f}%){bold_end}\")\n",
    "print(f\"Closed: {bold_start}{closed_count}{bold_end} times ({bold_start}{(closed_count / total_days) * 100:.2f}%){bold_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b06d93-59b8-4d2f-b1cc-d58b6504559b",
   "metadata": {},
   "source": [
    "#### 7. Check for zero Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda88f2c-b95a-46d4-a0b8-b2a63dde1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_sales_count = (train_df['sales'] == 0).sum()\n",
    "print(f\"\\nThere are {bold_start}{no_sales_count}{bold_end} times when stores made no sales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ab582-11ee-47d9-9708-f3606913944d",
   "metadata": {},
   "source": [
    "#### 8. Check for Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aefe83-250a-4d11-914a-c774fa7a3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_count = (train_df['sales'] != 0).sum()\n",
    "print(f\"\\nThere are {bold_start}{sales_count}{bold_end} times when stores recorded sales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e20e9f8-1480-46ae-8bf1-1e628f53299c",
   "metadata": {},
   "source": [
    "#### 9. Compare Sales vs No Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38cf99-e0ba-4c67-bb91-2d243f444750",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_days = len(train_df)\n",
    "\n",
    "print(f\"Sales: {bold_start}{sales_count}{bold_end}  times ({bold_start}{(sales_count / total_days) * 100:.2f}%){bold_end}\")\n",
    "print(f\"No-sales: {bold_start}{no_sales_count}{bold_end} times ({bold_start}{(no_sales_count / total_days) * 100:.2f}%){bold_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56e7c4-5aff-4d3b-85f3-d89c679c5851",
   "metadata": {},
   "source": [
    "#### 10. Customers Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf505c-b629-443f-8abf-790cc360fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_customers = len(train_df)\n",
    "no_customers_count = (train_df['customers'] == 0).sum()\n",
    "\n",
    "print(f\"\\nThere are {bold_start}{no_customers_count}{bold_end} times when no customers visited the stores.\")\n",
    "print(f\"No-Customers: {bold_start}{no_customers_count}{bold_end} on ({bold_start}{(no_customers_count / total_customers) * 100:.2f}%){bold_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a585af-96ca-49c6-b002-69508f5cd209",
   "metadata": {},
   "source": [
    "#### 11. Impact of Day of the Week - Average Customers and Sales by Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801a3a78-23fe-4360-b717-d69f09cb236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_analysis = (\n",
    "    train_df.groupby('dayofweek')[['customers', 'sales']]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'customers': 'avg_customers', 'sales': 'avg_sales'})\n",
    ")\n",
    "\n",
    "print(day_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073acc1c-77aa-4835-b75b-feb8fd0f1059",
   "metadata": {},
   "source": [
    "#### 12. Impact of Promotion - Average Customers and Sales by Promotion Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23210b99-59a8-45eb-abe3-1953c8407fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_analysis = (\n",
    "    train_df.groupby('promo')[['customers', 'sales']]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'customers': 'avg_customers', 'sales': 'avg_sales'})\n",
    ")\n",
    "\n",
    "print(promo_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c7c4e-b4b6-4151-819d-647b0e698398",
   "metadata": {},
   "source": [
    "#### 13. Day of Week Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30483fae-bb92-48b6-b76c-b6f2f4ba9283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average customers and sales by day of the week\n",
    "dow_analysis = (\n",
    "    train_df\n",
    "    .groupby(\"dayofweek\")[[\"customers\", \"sales\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"customers\": \"avg_customers\", \"sales\": \"avg_sales\"})\n",
    ")\n",
    "\n",
    "print(dow_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208e3cb-c139-4b1e-bb91-b384c55f205d",
   "metadata": {},
   "source": [
    "#### 14. SchoolHoliday Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7252c6d2-3f9f-4a3a-b8ce-bd801d9e6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "schoolholiday_analysis = (\n",
    "    train_df\n",
    "    .groupby(\"schoolholiday\")[[\"customers\", \"sales\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"customers\": \"avg_customers\", \"sales\": \"avg_sales\"})\n",
    ")\n",
    "\n",
    "print(schoolholiday_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a3a64-edd9-4539-83fb-20b4bad22878",
   "metadata": {},
   "source": [
    "#### 15. Top 10 Crowded Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a242ed97-9e37-4624-a73f-a27aff9b7762",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_crowded_store = (\n",
    "    train_df.groupby('store')['customers']\n",
    "    .mean()\n",
    "    .nlargest(10)\n",
    "    .reset_index()\n",
    "    .rename(columns={'customers': 'avg_customers'})\n",
    ")\n",
    "\n",
    "print(top10_crowded_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef3916-7e62-48af-a825-15fd21d8d0db",
   "metadata": {},
   "source": [
    "#### 16. Top 10 Highest-Selling Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6287c00-0354-473a-81f1-003adf46f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_selling_store = (\n",
    "    train_df.groupby('store')['sales']\n",
    "    .mean()\n",
    "    .nlargest(10)\n",
    "    .reset_index()\n",
    "    .rename(columns={'sales': 'avg_sales'})\n",
    ")\n",
    "\n",
    "print(top10_selling_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e5796-2080-4de1-b34a-48794d8351ec",
   "metadata": {},
   "source": [
    "#### 18. Crowded vs. Selling Stores - Sort by avg_sales in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a453ff-6d2b-4a58-925b-c3217e5ca56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = (\n",
    "    pd.merge(top10_crowded_store, top10_selling_store, on='store', how='outer')\n",
    "    .fillna(0)\n",
    "    .sort_values(by='avg_sales', ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228bffc3-2e6c-44db-9c54-5202a5103f95",
   "metadata": {},
   "source": [
    "#### 19. Dual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c987b-4bf4-4743-90a2-c7ac549ff922",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df['top_both'] = (\n",
    "    comparison_df['store'].isin(top10_crowded_store['store']) &\n",
    "    comparison_df['store'].isin(top10_selling_store['store'])\n",
    ")\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f1093-299f-4edd-80e7-6c34976481df",
   "metadata": {},
   "source": [
    "## 4. Features Engineering\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c6b500-81ee-4974-bd37-3894c0d10cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the original dataframe to avoid modifying it\n",
    "df_features = train_df.copy()\n",
    "\n",
    "# Ensure date column is in datetime format\n",
    "if not pd.api.types.is_datetime64_any_dtype(df_features['date']):\n",
    "    df_features['date'] = pd.to_datetime(df_features['date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e1a46e-d463-458a-8809-171598dc42c5",
   "metadata": {},
   "source": [
    "#### 4.1. Create Time-based Covariates -  Basic Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13657d9-bb2e-428f-a2cc-654305f3b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['day'] = df_features['date'].dt.strftime('%a')\n",
    "df_features['week'] = df_features['date'].dt.isocalendar().week\n",
    "df_features['month'] = df_features['date'].dt.strftime('%b')\n",
    "df_features['quarter'] = df_features['date'].dt.quarter\n",
    "df_features['year'] = df_features['date'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6518445-c9a5-45ec-9d5d-1adc3b192204",
   "metadata": {},
   "source": [
    "#### 4.2. StateHoliday Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ded3a6-d845-4778-87ba-50ae0e9a8752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the holiday type mapping\n",
    "holiday_map = {\n",
    "    \"0\": \"None\",\n",
    "    \"a\": \"Public\",\n",
    "    \"b\": \"Easter\",\n",
    "    \"c\": \"Christmas\"  \n",
    "}\n",
    "\n",
    "df_features['stateholiday']= df_features['stateholiday'].map(holiday_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ad2b7-6958-4671-992f-96a6d9945525",
   "metadata": {},
   "source": [
    "#### Promo mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92caf26-bda2-4f35-b865-f080e0d89adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['promo'] = df_features['promo'].astype(str).map({'1': 'Promo', '0': 'No Promo'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7908b-5c4e-424b-bc2c-a65ad115397b",
   "metadata": {},
   "source": [
    "#### Features Enginerring check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a6076b-84c9-456c-974e-fcf99b5e686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The shape after feature Engineering : {df_features.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788319eb-00ff-4093-9e10-60df00f344e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c51ea-f793-426c-b3a8-2a89127272ee",
   "metadata": {},
   "source": [
    "## Feature Engineering for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb12bf7-11ec-4a12-8dee-353293833826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for clean and unbiased data\n",
    "ts_train = train_df[(train_df['open'] == 1) & (train_df['sales'] > 0)].copy()\n",
    "\n",
    "# Sort by date\n",
    "ts_train.sort_values('date', ascending=True, inplace=True)\n",
    "\n",
    "# Ensure 'date' is datetime\n",
    "ts_train['date'] = pd.to_datetime(ts_train['date'])\n",
    "\n",
    "# Temporal features\n",
    "ts_train['dayofmonth'] = ts_train['date'].dt.day\n",
    "ts_train['dayofyear'] = ts_train['date'].dt.dayofyear\n",
    "ts_train['weekofyear'] = ts_train['date'].dt.isocalendar().week\n",
    "ts_train['month'] = ts_train['date'].dt.month\n",
    "ts_train['quarter'] = ts_train['date'].dt.quarter\n",
    "ts_train['year'] = ts_train['date'].dt.year\n",
    "\n",
    "# Cyclical features\n",
    "ts_train['day_sin'] = np.sin(2 * np.pi * ts_train['dayofweek'] / 7)\n",
    "ts_train['day_cos'] = np.cos(2 * np.pi * ts_train['dayofweek'] / 7)\n",
    "ts_train['month_sin'] = np.sin(2 * np.pi * ts_train['month'] / 12)\n",
    "ts_train['month_cos'] = np.cos(2 * np.pi * ts_train['month'] / 12)\n",
    "ts_train['week_sin'] = np.sin(2 * np.pi * ts_train['weekofyear'] / 52)\n",
    "ts_train['week_cos'] = np.cos(2 * np.pi * ts_train['weekofyear'] / 52)\n",
    "\n",
    "# Business features\n",
    "ts_train['isweekend'] = (ts_train['dayofweek'] > 5).astype(int)\n",
    "ts_train['ismonthstart'] = ts_train['date'].dt.is_month_start.astype(int)\n",
    "ts_train['ismonthend'] = ts_train['date'].dt.is_month_end.astype(int)\n",
    "ts_train['isquarterstart'] = ts_train['date'].dt.is_quarter_start.astype(int)\n",
    "ts_train['isquarterend'] = ts_train['date'].dt.is_quarter_end.astype(int)\n",
    "\n",
    "# Lag features\n",
    "for lag in [1, 2, 3, 7, 14, 30]:\n",
    "    ts_train[f'sales_lag_{lag}'] = ts_train.groupby('store')['sales'].shift(lag)\n",
    "\n",
    "# Rolling window features\n",
    "for window in [7, 14, 30]:\n",
    "    ts_train[f'sales_rolling_mean_{window}'] = (\n",
    "        ts_train.groupby('store')['sales'].rolling(window).mean().reset_index(level=0, drop=True)\n",
    "    )\n",
    "    ts_train[f'sales_rolling_std_{window}'] = (\n",
    "        ts_train.groupby('store')['sales'].rolling(window).std().reset_index(level=0, drop=True)\n",
    "    )\n",
    "    ts_train[f'sales_rolling_min_{window}'] = (\n",
    "        ts_train.groupby('store')['sales'].rolling(window).min().reset_index(level=0, drop=True)\n",
    "    )\n",
    "    ts_train[f'sales_rolling_max_{window}'] = (\n",
    "        ts_train.groupby('store')['sales'].rolling(window).max().reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Exponential moving averages\n",
    "for alpha in [0.1, 0.3, 0.5]:\n",
    "    ts_train[f'sales_ema_{alpha}'] = (\n",
    "        ts_train.groupby('store')['sales'].ewm(alpha=alpha).mean().reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Interaction features\n",
    "ts_train['promo_schoolholiday'] = ts_train['promo'] * ts_train['schoolholiday']\n",
    "ts_train['promo_stateholiday'] = ts_train['promo'] * ts_train['stateholiday']\n",
    "\n",
    "\n",
    "# Set date as index\n",
    "ts_train.set_index('date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb50e7c-f4b8-4078-81d6-2c9610c55003",
   "metadata": {},
   "source": [
    "#### 4.3. StateHoliday Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496247e1-a33f-4cfa-9104-5d75e1e43b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table for holiday impact\n",
    "stateholiday_analysis = (\n",
    "    df_features\n",
    "    .groupby(\"stateholiday\")[[\"customers\", \"sales\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"customers\": \"avg_customers\", \"sales\": \"avg_sales\"})\n",
    "    .sort_values(by=\"avg_sales\", ascending=False)\n",
    ")\n",
    "\n",
    "print(stateholiday_analysis)\n",
    "\n",
    "# Count times stores were closed during holidays (using temp labels)\n",
    "closed_holiday_days = df_features[(df_features[\"open\"] == 0) & (df_features.stateholiday != \"None\")].shape[0]\n",
    "print(f\"Number of closed times during holidays: {bold_start}{closed_holiday_days}{bold_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c19df8-8a2f-4810-8d11-13a4b68a3fe0",
   "metadata": {},
   "source": [
    "#### 4.4. Day & Seasonality Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209144f0-fbd7-41ea-a976-410804f36441",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_analysis = (\n",
    "    df_features\n",
    "    .groupby(\"day\")[[\"customers\",\"sales\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"customers\": \"avg_customers\",\"sales\": \"avg_sales\"})\n",
    "    .sort_values(by=\"avg_sales\", ascending=False)\n",
    ")\n",
    "\n",
    "print(day_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c56ac0-aec4-46ff-afac-8c8118b9352d",
   "metadata": {},
   "source": [
    "#### 4.5. Month & Seasonality Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f509d8e8-5300-4133-a5e1-3c26ce1b4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "month_analysis = (\n",
    "    df_features\n",
    "    .groupby(\"month\")[[\"customers\",\"sales\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"sales\": \"avg_sales\", \"customers\": \"avg_customers\"})\n",
    "    .sort_values(by=\"avg_sales\", ascending=False)\n",
    ")\n",
    "\n",
    "print(month_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795851d-8333-4227-9f7c-280c30e35c8f",
   "metadata": {},
   "source": [
    "#### 4.6. Year & Seasonality Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b2c369-3d60-48f3-ba08-c4fe19e420a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year_analysis = (\n",
    "    df_features\n",
    "    .groupby(\"year\")[[\"customers\", \"sales\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"sales\": \"avg_sales\", \"customers\": \"avg_customers\"})\n",
    "    .sort_values(by=\"avg_sales\", ascending=False)\n",
    ")\n",
    "\n",
    "print(year_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c731b370-60a0-4a4e-b4fd-ee84426a0329",
   "metadata": {},
   "source": [
    "#### 4.5. Promo √ó DayOfWeek Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa19dca-b5ee-4825-b390-1be8547bbc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_dow_analysis = (\n",
    "    df_features\n",
    "    .groupby([\"promo\", \"day\"])[[\"customers\", \"sales\",]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"sales\": \"avg_sales\", \"customers\": \"avg_customers\"})\n",
    "    .sort_values(by=\"avg_sales\", ascending=False)\n",
    ")\n",
    "\n",
    "print(promo_dow_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b146fa-a040-4799-b46c-d2a9ba93b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy to avoid modifying the original DataFrame\n",
    "df_temp = df_features.copy()\n",
    "\n",
    "# Insert promo_flag immediately after the 'promo' column\n",
    "promo_index = df_temp.columns.get_loc(\"promo\")\n",
    "df_temp.insert(promo_index + 1, \"promo_flag\", df_temp[\"promo\"] == 1)\n",
    "\n",
    "promo_dow_analysis = (\n",
    "    df_temp\n",
    "    .groupby([\"promo\", \"promo_flag\", \"day\"])[[\"customers\", \"sales\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"sales\": \"avg_sales\", \"customers\": \"avg_customers\"})\n",
    "    .sort_values(by=\"avg_sales\", ascending=False)\n",
    ")\n",
    "\n",
    "print(promo_dow_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dbaa0a-cb4f-4bd4-8796-17c0df794500",
   "metadata": {},
   "source": [
    "### Quarterly Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4635e9-d9a9-4927-979a-8e3c35025777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple and robust quarterly analysis\n",
    "quarter_avg = df_features.groupby('quarter')['sales'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Quarterly Sales Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Quarter':<10} {'Avg Sales':<12} {'Rank':<6} {'vs Q1':<8}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, (quarter, sales) in enumerate(quarter_avg.items(), 1):\n",
    "    vs_q1 = ((sales - quarter_avg[1]) / quarter_avg[1]) * 100\n",
    "    vs_q1_str = f\"{vs_q1:+.1f}%\" if quarter != 1 else \"Base\"\n",
    "    print(f\"Q{quarter:<9} ‚Ç¨{sales:>8,.0f}    {i:<6} {vs_q1_str:<8}\")\n",
    "\n",
    "print(f\"\\nQuarterly Summary:\")\n",
    "print(\"-\" * 20)\n",
    "best_q = quarter_avg.index[0]\n",
    "worst_q = quarter_avg.index[-1]\n",
    "range_pct = ((quarter_avg.max() - quarter_avg.min()) / quarter_avg.mean()) * 100\n",
    "\n",
    "print(f\"Best quarter: Q{best_q} (‚Ç¨{quarter_avg[best_q]:,.0f})\")\n",
    "print(f\"Worst quarter: Q{worst_q} (‚Ç¨{quarter_avg[worst_q]:,.0f})\")\n",
    "print(f\"Performance gap: {range_pct:.1f}%\")\n",
    "\n",
    "# Growth pattern\n",
    "print(f\"\\nQuarter-to-Quarter Growth:\")\n",
    "print(\"-\" * 25)\n",
    "for q in [2, 3, 4]:\n",
    "    growth = ((quarter_avg[q] - quarter_avg[q-1]) / quarter_avg[q-1]) * 100\n",
    "    print(f\"Q{q-1} to Q{q}: {growth:+.1f}%\")\n",
    "\n",
    "print(f\"\\nKey Insight: Q{best_q} generates {((quarter_avg[best_q]/quarter_avg.sum())*100):.1f}% of annual revenue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e726ba-7b9f-4f27-8270-666a57430978",
   "metadata": {},
   "source": [
    "# ‚ú® Pro Tips : Reusable Functions - Best Practices for Reusable Functions in Data Analysis\n",
    "-----------\n",
    "Reusable functions streamline analytical workflows by promoting consistency, reducing redundancy, and improving maintainability. To maximize their effectiveness:\n",
    "\n",
    " - Design for flexibility: Use parameters and config objects to adapt logic across datasets and use cases.\n",
    "\n",
    " - Keep functions atomic: Focus each function on a single task‚Äîcleaning, aggregating, visualizing, or exporting.\n",
    "\n",
    " - Avoid side effects: Return outputs explicitly; defer file I/O or plotting to higher-level orchestration.\n",
    "\n",
    " - Document clearly: Use concise docstrings and intuitive naming for better readability and future reuse.\n",
    "\n",
    " - Centralize configuration: Store defaults and settings in external files or global dictionaries for easy updates.\n",
    "\n",
    " - Efficient function design leads to cleaner notebooks, faster iteration, and scalable analysis pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574dab5-f9a9-4670-b2a3-754052bc1fe8",
   "metadata": {},
   "source": [
    "# üîß Generalized Reusable Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d418e-05aa-41df-96ad-34f5f45330d0",
   "metadata": {},
   "source": [
    "## Impact promo analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997a27e3-64bd-44d3-b440-09954ede4710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def clean_promo_analysis(df, sales_col='sales', customers_col='customers', \n",
    "                        store_col='store', promo_col='promo', date_col='date', top_n=10):\n",
    "    \"\"\"\n",
    "    Clean and comprehensive promotional impact analysis\n",
    "    \"\"\"\n",
    "    print(\"üéØ PROMOTIONAL IMPACT ANALYSIS REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Data preprocessing\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remove closed stores (sales = 0)\n",
    "    df_clean = df_clean[df_clean[sales_col] > 0]\n",
    "    print(f\"üìä Data Overview: {len(df_clean):,} records after removing closed days\")\n",
    "    \n",
    "    # Create binary promo flag\n",
    "    df_clean['promo_flag'] = (df_clean[promo_col] == 'Promo').astype(int)\n",
    "    \n",
    "    # Get top stores by average sales\n",
    "    top_stores = df_clean.groupby(store_col)[sales_col].mean().nlargest(top_n).index\n",
    "    df_analysis = df_clean[df_clean[store_col].isin(top_stores)]\n",
    "    \n",
    "    print(f\"üè™ Analyzing top {len(top_stores)} stores: {list(top_stores)}\")\n",
    "    print(f\"üìà Analysis dataset: {len(df_analysis):,} records\")\n",
    "    \n",
    "    # Split data\n",
    "    promo_data = df_analysis[df_analysis['promo_flag'] == 1]\n",
    "    non_promo_data = df_analysis[df_analysis['promo_flag'] == 0]\n",
    "    \n",
    "    print(f\"üéØ Promotional days: {len(promo_data):,} ({len(promo_data)/len(df_analysis)*100:.1f}%)\")\n",
    "    print(f\"üìÖ Regular days: {len(non_promo_data):,} ({len(non_promo_data)/len(df_analysis)*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    results = {}\n",
    "    \n",
    "    # Sales metrics\n",
    "    promo_avg_sales = promo_data[sales_col].mean()\n",
    "    non_promo_avg_sales = non_promo_data[sales_col].mean()\n",
    "    sales_lift = promo_avg_sales - non_promo_avg_sales\n",
    "    sales_lift_pct = (sales_lift / non_promo_avg_sales) * 100\n",
    "    \n",
    "    # Customer metrics  \n",
    "    promo_avg_customers = promo_data[customers_col].mean()\n",
    "    non_promo_avg_customers = non_promo_data[customers_col].mean()\n",
    "    customer_lift = promo_avg_customers - non_promo_avg_customers\n",
    "    customer_lift_pct = (customer_lift / non_promo_avg_customers) * 100\n",
    "    \n",
    "    # Efficiency metrics\n",
    "    promo_sales_per_customer = promo_avg_sales / promo_avg_customers\n",
    "    non_promo_sales_per_customer = non_promo_avg_sales / non_promo_avg_customers\n",
    "    efficiency_improvement = ((promo_sales_per_customer - non_promo_sales_per_customer) / \n",
    "                             non_promo_sales_per_customer) * 100\n",
    "    \n",
    "    # Statistical test\n",
    "    t_stat, p_value = ttest_ind(promo_data[sales_col], non_promo_data[sales_col])\n",
    "    is_significant = p_value < 0.05\n",
    "    \n",
    "    # Store-level analysis\n",
    "    store_results = []\n",
    "    for store in top_stores:\n",
    "        store_data = df_analysis[df_analysis[store_col] == store]\n",
    "        store_promo = store_data[store_data['promo_flag'] == 1]\n",
    "        store_regular = store_data[store_data['promo_flag'] == 0]\n",
    "        \n",
    "        if len(store_promo) > 0 and len(store_regular) > 0:\n",
    "            store_sales_lift = ((store_promo[sales_col].mean() - store_regular[sales_col].mean()) / \n",
    "                              store_regular[sales_col].mean()) * 100\n",
    "            store_customer_lift = ((store_promo[customers_col].mean() - store_regular[customers_col].mean()) / \n",
    "                                 store_regular[customers_col].mean()) * 100\n",
    "            \n",
    "            store_results.append({\n",
    "                'Store': store,\n",
    "                'Promo Days': len(store_promo),\n",
    "                'Regular Days': len(store_regular),\n",
    "                'Promo Rate (%)': len(store_promo) / len(store_data) * 100,\n",
    "                'Sales Lift (%)': store_sales_lift,\n",
    "                'Customer Lift (%)': store_customer_lift,\n",
    "                'Promo Avg Sales': store_promo[sales_col].mean(),\n",
    "                'Regular Avg Sales': store_regular[sales_col].mean(),\n",
    "                'Promo Avg Customers': store_promo[customers_col].mean(),\n",
    "                'Regular Avg Customers': store_regular[customers_col].mean()\n",
    "            })\n",
    "    \n",
    "    store_df = pd.DataFrame(store_results)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nüí∞ SALES PERFORMANCE ANALYSIS\")\n",
    "    print(f\"=\"*40)\n",
    "    print(f\"üéØ Average Sales (Promotional): ${promo_avg_sales:,.0f}\")\n",
    "    print(f\"üìä Average Sales (Regular): ${non_promo_avg_sales:,.0f}\")\n",
    "    print(f\"‚¨ÜÔ∏è  Absolute Sales Lift: ${sales_lift:,.0f}\")\n",
    "    print(f\"üìà Percentage Sales Lift: +{sales_lift_pct:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nüë• CUSTOMER TRAFFIC ANALYSIS\") \n",
    "    print(f\"=\"*40)\n",
    "    print(f\"üéØ Average Customers (Promotional): {promo_avg_customers:,.0f}\")\n",
    "    print(f\"üìä Average Customers (Regular): {non_promo_avg_customers:,.0f}\")\n",
    "    print(f\"‚¨ÜÔ∏è  Customer Traffic Lift: +{customer_lift:.0f}\")\n",
    "    print(f\"üìà Customer Traffic Lift: +{customer_lift_pct:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nüéØ EFFICIENCY & PROFITABILITY\")\n",
    "    print(f\"=\"*40)\n",
    "    print(f\"üí≥ Sales per Customer (Promotional): ${promo_sales_per_customer:.2f}\")\n",
    "    print(f\"üí≥ Sales per Customer (Regular): ${non_promo_sales_per_customer:.2f}\")\n",
    "    print(f\"üìä Spending Efficiency Gain: +{efficiency_improvement:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nüìä STATISTICAL VALIDATION\")\n",
    "    print(f\"=\"*40)\n",
    "    print(f\"üßÆ T-Statistic: {t_stat:.2f}\")\n",
    "    print(f\"üìà P-Value: {p_value:.6f}\")\n",
    "    print(f\"‚úÖ Statistically Significant: {'YES' if is_significant else 'NO'} (Œ±=0.05)\")\n",
    "    \n",
    "    # Business insights\n",
    "    print(f\"\\nüí° KEY BUSINESS INSIGHTS\")\n",
    "    print(f\"=\"*40)\n",
    "    \n",
    "    if sales_lift_pct > 50:\n",
    "        print(f\"üöÄ EXCEPTIONAL PERFORMANCE: Promotions drive outstanding sales growth!\")\n",
    "        recommendation = \"MAXIMIZE promotional frequency - ROI is excellent\"\n",
    "    elif sales_lift_pct > 25:\n",
    "        print(f\"‚úÖ STRONG PERFORMANCE: Promotions are highly effective\")\n",
    "        recommendation = \"INCREASE promotional activities strategically\"\n",
    "    elif sales_lift_pct > 10:\n",
    "        print(f\"üëç GOOD PERFORMANCE: Promotions show solid results\")\n",
    "        recommendation = \"MAINTAIN current promotional strategy\"\n",
    "    elif sales_lift_pct > 0:\n",
    "        print(f\"‚ö†Ô∏è  WEAK PERFORMANCE: Minimal promotional benefit\")\n",
    "        recommendation = \"REVIEW promotional costs vs benefits\"\n",
    "    else:\n",
    "        print(f\"‚ùå NEGATIVE IMPACT: Promotions may be hurting performance\")\n",
    "        recommendation = \"URGENT REVIEW of promotional strategy needed\"\n",
    "    \n",
    "    print(f\"üìã RECOMMENDATION: {recommendation}\")\n",
    "    \n",
    "    # Traffic vs Spending analysis\n",
    "    if customer_lift_pct > efficiency_improvement:\n",
    "        print(f\"üë• PRIMARY DRIVER: Promotions mainly drive FOOT TRAFFIC (+{customer_lift_pct:.1f}%)\")\n",
    "        print(f\"   ‚Üí Focus on conversion and upselling during promotions\")\n",
    "    elif efficiency_improvement > customer_lift_pct:\n",
    "        print(f\"üí∞ PRIMARY DRIVER: Promotions increase SPENDING PER VISIT (+{efficiency_improvement:.1f}%)\")\n",
    "        print(f\"   ‚Üí Excellent basket size improvement\")\n",
    "    else:\n",
    "        print(f\"‚öñÔ∏è  BALANCED IMPACT: Both traffic and spending improve equally\")\n",
    "    \n",
    "    # Store performance insights\n",
    "    if not store_df.empty:\n",
    "        best_store = store_df.loc[store_df['Sales Lift (%)'].idxmax()]\n",
    "        worst_store = store_df.loc[store_df['Sales Lift (%)'].idxmin()]\n",
    "        \n",
    "        print(f\"\\nüèÜ TOP PERFORMING STORE: #{int(best_store['Store'])}\")\n",
    "        print(f\"   üìà Sales Lift: +{best_store['Sales Lift (%)']:.1f}%\")\n",
    "        print(f\"   üë• Customer Lift: +{best_store['Customer Lift (%)']:.1f}%\")\n",
    "        print(f\"   üéØ Promo Rate: {best_store['Promo Rate (%)']:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nüìâ LOWEST PERFORMING STORE: #{int(worst_store['Store'])}\")\n",
    "        print(f\"   üìà Sales Lift: +{best_store['Sales Lift (%)']:.1f}%\")\n",
    "        print(f\"   üë• Customer Lift: +{worst_store['Customer Lift (%)']:.1f}%\")\n",
    "        print(f\"   üéØ Promo Rate: {worst_store['Promo Rate (%)']:.1f}%\")\n",
    "        \n",
    "        avg_lift = store_df['Sales Lift (%)'].mean()\n",
    "        consistent_stores = ((store_df['Sales Lift (%)'] - avg_lift).abs() < 10).sum()\n",
    "        \n",
    "        print(f\"\\nüìä CONSISTENCY ANALYSIS:\")\n",
    "        print(f\"   üéØ Average Lift Across Stores: +{avg_lift:.1f}%\")\n",
    "        print(f\"   üìè Performance Consistency: {consistent_stores}/{len(store_df)} stores within ¬±10%\")\n",
    "        \n",
    "        if consistent_stores / len(store_df) > 0.8:\n",
    "            print(f\"   ‚úÖ HIGHLY CONSISTENT: Promotions work well across all stores\")\n",
    "        elif consistent_stores / len(store_df) > 0.6:\n",
    "            print(f\"   üëç MODERATELY CONSISTENT: Most stores benefit similarly\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  INCONSISTENT: Results vary significantly by store\")\n",
    "            print(f\"   ‚Üí Investigate store-specific factors affecting promotional performance\")\n",
    "    \n",
    "    # Time-based insights (if date available)\n",
    "    if date_col in df_analysis.columns:\n",
    "        df_analysis['month'] = pd.to_datetime(df_analysis[date_col]).dt.month_name()\n",
    "        df_analysis['weekday'] = pd.to_datetime(df_analysis[date_col]).dt.day_name()\n",
    "        \n",
    "        # Monthly performance\n",
    "        monthly_promo = df_analysis[df_analysis['promo_flag']==1].groupby('month')[sales_col].mean()\n",
    "        monthly_regular = df_analysis[df_analysis['promo_flag']==0].groupby('month')[sales_col].mean()\n",
    "        monthly_lift = ((monthly_promo - monthly_regular) / monthly_regular * 100).round(1)\n",
    "        \n",
    "        best_month = monthly_lift.idxmax()\n",
    "        worst_month = monthly_lift.idxmin()\n",
    "        \n",
    "        print(f\"\\nüìÖ SEASONAL INSIGHTS:\")\n",
    "        print(f\"   üèÜ Best Month for Promos: {best_month} (+{monthly_lift[best_month]:.1f}%)\")\n",
    "        print(f\"   üìâ Worst Month for Promos: {worst_month} (+{monthly_lift[worst_month]:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüéâ ANALYSIS COMPLETE!\")\n",
    "    \n",
    "    # Return structured results\n",
    "    return {\n",
    "        'summary_metrics': {\n",
    "            'sales_lift_pct': sales_lift_pct,\n",
    "            'customer_lift_pct': customer_lift_pct,\n",
    "            'efficiency_improvement': efficiency_improvement,\n",
    "            'statistical_significance': is_significant,\n",
    "            'p_value': p_value\n",
    "        },\n",
    "        'store_performance': store_df,\n",
    "        'raw_data': {\n",
    "            'promo_avg_sales': promo_avg_sales,\n",
    "            'regular_avg_sales': non_promo_avg_sales,\n",
    "            'promo_avg_customers': promo_avg_customers,\n",
    "            'regular_avg_customers': non_promo_avg_customers\n",
    "        }\n",
    "    }\n",
    "\n",
    "def create_promo_visualization(results_dict, store_df):\n",
    "    \"\"\"\n",
    "    Create visualizations for promotional analysis\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Sales comparison\n",
    "    metrics = ['Promo', 'Regular']\n",
    "    sales_values = [results_dict['raw_data']['promo_avg_sales'], \n",
    "                   results_dict['raw_data']['regular_avg_sales']]\n",
    "    \n",
    "    bars1 = ax1.bar(metrics, sales_values, color=['#ff6b6b', '#4ecdc4'], alpha=0.8)\n",
    "    ax1.set_title('Average Sales: Promotional vs Regular Days', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Average Sales ($)')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'${height:,.0f}', ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    # 2. Customer traffic comparison\n",
    "    customer_values = [results_dict['raw_data']['promo_avg_customers'],\n",
    "                      results_dict['raw_data']['regular_avg_customers']]\n",
    "    \n",
    "    bars2 = ax2.bar(metrics, customer_values, color=['#ff9f43', '#54a0ff'], alpha=0.8)\n",
    "    ax2.set_title('Average Customer Traffic: Promotional vs Regular Days', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Average Customers')\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:,.0f}', ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    # 3. Store performance distribution\n",
    "    if not store_df.empty:\n",
    "        ax3.hist(store_df['Sales Lift (%)'], bins=8, alpha=0.7, color='#ff6b6b', edgecolor='black')\n",
    "        ax3.set_title('Distribution of Sales Lift Across Stores', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Sales Lift (%)')\n",
    "        ax3.set_ylabel('Number of Stores')\n",
    "        ax3.axvline(store_df['Sales Lift (%)'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {store_df[\"Sales Lift (%)\"].mean():.1f}%')\n",
    "        ax3.legend()\n",
    "    \n",
    "    # 4. Key metrics summary\n",
    "    lift_pct = results_dict['summary_metrics']['sales_lift_pct']\n",
    "    customer_lift_pct = results_dict['summary_metrics']['customer_lift_pct']\n",
    "    efficiency = results_dict['summary_metrics']['efficiency_improvement']\n",
    "    \n",
    "    metrics_names = ['Sales Lift', 'Customer Lift', 'Efficiency Gain']\n",
    "    metrics_values = [lift_pct, customer_lift_pct, efficiency]\n",
    "    colors = ['#ff6b6b', '#4ecdc4', '#45aaf2']\n",
    "    \n",
    "    bars4 = ax4.bar(metrics_names, metrics_values, color=colors, alpha=0.8)\n",
    "    ax4.set_title('Key Performance Metrics (%)', fontsize=14, fontweight='bold')\n",
    "    ax4.set_ylabel('Improvement (%)')\n",
    "    \n",
    "    for bar in bars4:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'+{height:.1f}%', ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "results = clean_promo_analysis(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b6b1d-ba96-4299-a935-6d4a8bcfcfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pull train_df from one notebook to another in JupyterLab\n",
    "%store train_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b33402-11dd-471a-b9c2-effe7f65283b",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734fdac-47a4-4b96-9633-c77a9fd333be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Data Ingestion and Exploratory Data Analysis completed successfully!\")\n",
    "print(f\"üóìÔ∏è Analysis Date: {bold_start}{pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}{bold_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb3d94-675d-4cea-bc83-2e995e1e8c94",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a578a31-6bf1-4605-8d14-c69c3df0809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End analysis\n",
    "analysis_end = pd.Timestamp.now()\n",
    "duration = analysis_end - analysis_begin\n",
    "\n",
    "# Final summary print\n",
    "print(\"\\nüìã Analysis Summary\")\n",
    "print(f\"üü¢ Begin Date: {bold_start}{analysis_begin.strftime('%Y-%m-%d %H:%M:%S')}{bold_end}\")\n",
    "print(f\"‚úÖ End Date:   {bold_start}{analysis_end.strftime('%Y-%m-%d %H:%M:%S')}{bold_end}\")\n",
    "print(f\"‚è±Ô∏è Duration:   {bold_start}{str(duration)}{bold_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045b4131-f856-44b5-81e8-1f72274952fa",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "## Project Design Rationale: Notebook Separation\n",
    "\n",
    "To promote **clarity, maintainability, and scalability** within the project, **data engineering** and **visualization tasks** are intentionally separated into distinct notebooks. This modular approach prevents the accumulation of excessive code in a single notebook, making it easier to **debug, update, and collaborate across different stages of the workflow**. By isolating data transformation logic from visual analysis, **each notebook remains focused and purpose-driven**, ultimately **enhancing the overall efficiency and readability of the project**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a5140-e2f8-414a-bea9-7613a656a305",
   "metadata": {},
   "source": [
    "\n",
    "ARNAUD DAVY - MUKWA NDUDI\n",
    "-----------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
