{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ef5700-bd71-4e44-99c4-6f17485d94f5",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Feature Engineering Overview ‚Äì Rossmann Store Sales\n",
    "\n",
    "This step focused on transforming raw inputs into meaningful features that capture temporal patterns, store-specific characteristics, and external influences‚Äîessential for improving model accuracy in forecasting daily sales.\n",
    "Key Feature Engineering Strategies:\n",
    "\n",
    "- **Date-based decomposition:** Extracted features like day of week, month, year, and week of year to model seasonality and calendar effects.\n",
    "\n",
    "- **Lag and rolling statistics:** Introduced lagged sales values and rolling averages to reflect short-term trends and autocorrelation.\n",
    "\n",
    "- **Store-level attributes:** Included static features such as store type, assortment level, and competition distance to differentiate store behavior.\n",
    "\n",
    "- **Promotion indicators:** Engineered features from Promo, Promo2, and PromoInterval to capture promotional impact over time.\n",
    "\n",
    "- **Competition and holiday effects:** Added flags and time-since metrics for competition openings and holidays to account for external disruptions.\n",
    "\n",
    "- **Interaction terms:** Created combined features (e.g., StoreType √ó DayOfWeek) to capture complex relationships.\n",
    "\n",
    "By enriching the dataset with these engineered features, the model is better equipped to learn from historical patterns and contextual signals, ultimately enhancing predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd832622-b6c7-4b7f-a4ef-26eeea11e6a1",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports Libraries\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a4ece7-9e78-4878-b52b-28605ad85ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4563f70e-b121-4e17-8522-e90be7726f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Setup and Import Libraries in progress...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Data Ingestion\n",
    "print(\"Step 1: Setup and Import Libraries in progress...\")\n",
    "time.sleep(1)  # Simulate processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75d6c5fa-aee0-4e1f-9e59-840fdba88e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Rossman Store Sales Time Series Analysis - Part 1\n",
      "============================================================\n",
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data Manipulation & Processing\n",
    "import os\n",
    "import holidays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import scipy.stats as stats\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format','{:.2f}'.format)\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Rossman Store Sales Time Series Analysis - Part 1\")\n",
    "print(\"=\"*60)\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28356173-747b-4d1f-8398-e5ccca45e36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup and Import Liraries completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Setup and Import Liraries completed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cce5b68-7d10-4d23-aa9f-34b449a511ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analysis Started\n",
      "üü¢ Begin Date: \u001b[1m2025-08-13 21:19:58\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start analysis\n",
    "\n",
    "analysis_begin = pd.Timestamp.now()\n",
    "\n",
    "bold_start = '\\033[1m'\n",
    "bold_end = '\\033[0m'\n",
    "\n",
    "print(\"üîç Analysis Started\")\n",
    "print(f\"üü¢ Begin Date: {bold_start}{analysis_begin.strftime('%Y-%m-%d %H:%M:%S')}{bold_end}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca539c-3bf4-4d74-bc5a-0b4a77793146",
   "metadata": {},
   "source": [
    "\n",
    "## Restore file\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "705d6ee5-e88f-43e5-89fd-6e454bfb56d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Data Ingestion in progress...\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Data Ingestion\n",
    "print(\"Step 2: Data Ingestion in progress...\")\n",
    "time.sleep(1)  # Simulate processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62bfa428-3b5f-4d36-8d1b-100c53ba44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pull df_features from one notebook to another in JupyterLab\n",
    "%store -r train_df\n",
    "%store -r df_viz_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fe1b626-5584-493e-89b2-4b13da6daec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Ingestion completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Data Ingestion completed.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988d899-2b50-45c1-82de-8e6e2c196546",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis (EDA)\n",
    "## 3.1. Basic Inspection\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48fb8f22-7014-4523-8a1b-7a6338f396a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Step 3: Exploratory Data Analysis in progress...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Exploratory Data Analysis (EDA)\n",
    "print(\"üìä Step 3: Exploratory Data Analysis in progress...\")\n",
    "time.sleep(1)  # Simulate processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4157b20d-8631-44d4-9512-0f305dbd184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 982644 entries, 982643 to 0\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   store          982644 non-null  int64 \n",
      " 1   dayofweek      982644 non-null  int64 \n",
      " 2   date           982644 non-null  object\n",
      " 3   sales          982644 non-null  int64 \n",
      " 4   customers      982644 non-null  int64 \n",
      " 5   open           982644 non-null  int64 \n",
      " 6   promo          982644 non-null  int64 \n",
      " 7   stateholiday   982644 non-null  object\n",
      " 8   schoolholiday  982644 non-null  int64 \n",
      "dtypes: int64(7), object(2)\n",
      "memory usage: 75.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.columns = train_df.columns.str.lower()\n",
    "\n",
    "# --- BASIC INFO AND DUPLICATES ---\n",
    "print(\"DataFrame Info:\")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c51ea-f793-426c-b3a8-2a89127272ee",
   "metadata": {},
   "source": [
    "## Feature Engineering for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eb12bf7-11ec-4a12-8dee-353293833826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for clean and unbiased data\n",
    "ts_train = train_df[(train_df['open'] == 1) & (train_df['sales'] > 0)].copy()\n",
    "\n",
    "# Sort by date\n",
    "ts_train.sort_values('date', ascending=True, inplace=True)\n",
    "\n",
    "# Ensure 'date' is datetime\n",
    "ts_train['date'] = pd.to_datetime(ts_train['date'])\n",
    "\n",
    "# Temporal features\n",
    "ts_train['dayofmonth'] = ts_train['date'].dt.day\n",
    "ts_train['dayofyear'] = ts_train['date'].dt.dayofyear\n",
    "ts_train['weekofyear'] = ts_train['date'].dt.isocalendar().week\n",
    "ts_train['month'] = ts_train['date'].dt.month\n",
    "ts_train['quarter'] = ts_train['date'].dt.quarter\n",
    "ts_train['year'] = ts_train['date'].dt.year\n",
    "\n",
    "# Cyclical features\n",
    "ts_train['day_sin'] = np.sin(2 * np.pi * ts_train['dayofweek'] / 7)\n",
    "ts_train['day_cos'] = np.cos(2 * np.pi * ts_train['dayofweek'] / 7)\n",
    "ts_train['month_sin'] = np.sin(2 * np.pi * ts_train['month'] / 12)\n",
    "ts_train['month_cos'] = np.cos(2 * np.pi * ts_train['month'] / 12)\n",
    "ts_train['week_sin'] = np.sin(2 * np.pi * ts_train['weekofyear'] / 52)\n",
    "ts_train['week_cos'] = np.cos(2 * np.pi * ts_train['weekofyear'] / 52)\n",
    "\n",
    "# Business features\n",
    "ts_train['isweekend'] = (ts_train['dayofweek'] > 5).astype(int)\n",
    "ts_train['ismonthstart'] = ts_train['date'].dt.is_month_start.astype(int)\n",
    "ts_train['ismonthend'] = ts_train['date'].dt.is_month_end.astype(int)\n",
    "ts_train['isquarterstart'] = ts_train['date'].dt.is_quarter_start.astype(int)\n",
    "ts_train['isquarterend'] = ts_train['date'].dt.is_quarter_end.astype(int)\n",
    "\n",
    "# Lag features\n",
    "for lag in [1, 2, 3, 7, 14, 30]:\n",
    "    ts_train[f'sales_lag_{lag}'] = ts_train.groupby('store')['sales'].shift(lag)\n",
    "\n",
    "# Rolling window features\n",
    "for window in [7, 14, 30]:\n",
    "    ts_train[f'sales_rolling_mean_{window}'] = (\n",
    "        ts_train.groupby('store')['sales'].rolling(window).mean().reset_index(level=0, drop=True)\n",
    "    )\n",
    "    ts_train[f'sales_rolling_std_{window}'] = (\n",
    "        ts_train.groupby('store')['sales'].rolling(window).std().reset_index(level=0, drop=True)\n",
    "    )\n",
    "    ts_train[f'sales_rolling_min_{window}'] = (\n",
    "        ts_train.groupby('store')['sales'].rolling(window).min().reset_index(level=0, drop=True)\n",
    "    )\n",
    "    ts_train[f'sales_rolling_max_{window}'] = (\n",
    "        ts_train.groupby('store')['sales'].rolling(window).max().reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Exponential moving averages\n",
    "for alpha in [0.1, 0.3, 0.5]:\n",
    "    ts_train[f'sales_ema_{alpha}'] = (\n",
    "        ts_train.groupby('store')['sales'].ewm(alpha=alpha).mean().reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Interaction features\n",
    "ts_train['promo_schoolholiday'] = ts_train['promo'] * ts_train['schoolholiday']\n",
    "ts_train['promo_stateholiday'] = ts_train['promo'] * ts_train['stateholiday']\n",
    "\n",
    "\n",
    "# Set date as index\n",
    "ts_train.set_index('date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb50e7c-f4b8-4078-81d6-2c9610c55003",
   "metadata": {},
   "source": [
    "#### 4.3. StateHoliday Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "496247e1-a33f-4cfa-9104-5d75e1e43b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  stateholiday  avg_customers  avg_sales\n",
      "2   Normal Day         652.11    5940.39\n",
      "3       Public          43.82     290.74\n",
      "1       Easter          36.56     214.31\n",
      "0    Christmas          27.17     168.73\n",
      "Number of closed times during holidays: \u001b[1m168440\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create summary table for holiday impact\n",
    "stateholiday_analysis = (\n",
    "    df_viz_feat\n",
    "    .groupby(\"stateholiday\")[[\"customers\", \"sales\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"customers\": \"avg_customers\", \"sales\": \"avg_sales\"})\n",
    "    .sort_values(by=\"avg_sales\", ascending=False)\n",
    ")\n",
    "\n",
    "print(stateholiday_analysis)\n",
    "\n",
    "# Count times stores were closed during holidays (using temp labels)\n",
    "closed_holiday_days = df_viz_feat[(df_viz_feat[\"open\"] == 0) & (df_viz_feat.stateholiday != \"None\")].shape[0]\n",
    "print(f\"Number of closed times during holidays: {bold_start}{closed_holiday_days}{bold_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c19df8-8a2f-4810-8d11-13a4b68a3fe0",
   "metadata": {},
   "source": [
    "#### 4.4. Day & Seasonality Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "209144f0-fbd7-41ea-a976-410804f36441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   day  avg_customers  avg_sales\n",
      "1  Mon         812.93    7797.64\n",
      "5  Tue         761.86    7005.52\n",
      "0  Fri         742.53    6703.50\n",
      "6  Wed         721.20    6536.45\n",
      "4  Thu         695.78    6216.11\n",
      "2  Sat         658.76    5856.78\n",
      "3  Sun          35.58     202.62\n"
     ]
    }
   ],
   "source": [
    "day_analysis = (\n",
    "    df_viz_feat\n",
    "    .groupby(\"day\")[[\"customers\",\"sales\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"customers\": \"avg_customers\",\"sales\": \"avg_sales\"})\n",
    "    .sort_values(by=\"avg_sales\", ascending=False)\n",
    ")\n",
    "\n",
    "print(day_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c56ac0-aec4-46ff-afac-8c8118b9352d",
   "metadata": {},
   "source": [
    "#### 4.5. Month & Seasonality Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f509d8e8-5300-4133-a5e1-3c26ce1b4fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   month  avg_customers  avg_sales\n",
      "2    Dec         703.07    6826.61\n",
      "5    Jul         663.59    6022.61\n",
      "9    Nov         654.15    6008.11\n",
      "7    Mar         629.40    5784.58\n",
      "6    Jun         624.79    5760.96\n",
      "0    Apr         630.61    5738.87\n",
      "1    Aug         642.50    5693.02\n",
      "3    Feb         626.72    5645.25\n",
      "11   Sep         634.44    5570.25\n",
      "10   Oct         631.10    5537.04\n",
      "8    May         601.99    5489.64\n",
      "4    Jan         601.62    5465.40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "month_analysis = (\n",
    "    df_viz_feat\n",
    "    .groupby(\"month\")[[\"customers\",\"sales\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"sales\": \"avg_sales\", \"customers\": \"avg_customers\"})\n",
    "    .sort_values(by=\"avg_sales\", ascending=False)\n",
    ")\n",
    "\n",
    "print(month_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795851d-8333-4227-9f7c-280c30e35c8f",
   "metadata": {},
   "source": [
    "#### 4.6. Year & Seasonality Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8b2c369-3d60-48f3-ba08-c4fe19e420a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  avg_customers  avg_sales\n",
      "1  2014         643.27    5833.29\n",
      "2  2015         620.84    5832.95\n",
      "0  2013         629.04    5658.53\n"
     ]
    }
   ],
   "source": [
    "\n",
    "year_analysis = (\n",
    "    df_viz_feat\n",
    "    .groupby(\"year\")[[\"customers\", \"sales\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"sales\": \"avg_sales\", \"customers\": \"avg_customers\"})\n",
    "    .sort_values(by=\"avg_sales\", ascending=False)\n",
    ")\n",
    "\n",
    "print(year_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c731b370-60a0-4a4e-b4fd-ee84426a0329",
   "metadata": {},
   "source": [
    "#### 4.5. Promo √ó DayOfWeek Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fa19dca-b5ee-4825-b390-1be8547bbc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       promo  day  avg_customers  avg_sales\n",
      "8      Promo  Mon         938.67    9709.13\n",
      "10     Promo  Tue         837.47    8226.49\n",
      "11     Promo  Wed         785.91    7540.85\n",
      "9      Promo  Thu         774.00    7241.50\n",
      "7      Promo  Fri         765.53    7168.90\n",
      "0   No Promo  Fri         716.68    6180.31\n",
      "2   No Promo  Sat         658.76    5856.78\n",
      "5   No Promo  Tue         675.35    5608.49\n",
      "1   No Promo  Mon         666.24    5567.56\n",
      "6   No Promo  Wed         648.26    5404.23\n",
      "4   No Promo  Thu         607.85    5063.39\n",
      "3   No Promo  Sun          35.58     202.62\n"
     ]
    }
   ],
   "source": [
    "promo_dow_analysis = (\n",
    "    df_viz_feat\n",
    "    .groupby([\"promo\", \"day\"])[[\"customers\", \"sales\",]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"sales\": \"avg_sales\", \"customers\": \"avg_customers\"})\n",
    "    .sort_values(by=\"avg_sales\", ascending=False)\n",
    ")\n",
    "\n",
    "print(promo_dow_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1b146fa-a040-4799-b46c-d2a9ba93b5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       promo  promo_flag  day  avg_customers  avg_sales\n",
      "8      Promo        True  Mon         938.67    9709.13\n",
      "10     Promo        True  Tue         837.47    8226.49\n",
      "11     Promo        True  Wed         785.91    7540.85\n",
      "9      Promo        True  Thu         774.00    7241.50\n",
      "7      Promo        True  Fri         765.53    7168.90\n",
      "0   No Promo       False  Fri         716.68    6180.31\n",
      "2   No Promo       False  Sat         658.76    5856.78\n",
      "5   No Promo       False  Tue         675.35    5608.49\n",
      "1   No Promo       False  Mon         666.24    5567.56\n",
      "6   No Promo       False  Wed         648.26    5404.23\n",
      "4   No Promo       False  Thu         607.85    5063.39\n",
      "3   No Promo       False  Sun          35.58     202.62\n"
     ]
    }
   ],
   "source": [
    "# Create a copy to avoid modifying the original DataFrame\n",
    "df_temp = df_viz_feat.copy()\n",
    "\n",
    "# Insert promo_flag immediately after the 'promo' column\n",
    "promo_index = df_temp.columns.get_loc(\"promo\")\n",
    "df_temp.insert(promo_index + 1, \"promo_flag\", df_temp[\"promo\"] == \"Promo\")\n",
    "\n",
    "promo_dow_analysis = (\n",
    "    df_temp\n",
    "    .groupby([\"promo\", \"promo_flag\", \"day\"])[[\"customers\", \"sales\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"sales\": \"avg_sales\", \"customers\": \"avg_customers\"})\n",
    "    .sort_values(by=\"avg_sales\", ascending=False)\n",
    ")\n",
    "\n",
    "print(promo_dow_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dbaa0a-cb4f-4bd4-8796-17c0df794500",
   "metadata": {},
   "source": [
    "### Quarterly Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e4635e9-d9a9-4927-979a-8e3c35025777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarterly Sales Analysis:\n",
      "==================================================\n",
      "Quarter    Avg Sales    Rank   vs Q1   \n",
      "--------------------------------------------------\n",
      "Q4         ‚Ç¨   6,125    1      +8.8%   \n",
      "Q3         ‚Ç¨   5,764    2      +2.4%   \n",
      "Q2         ‚Ç¨   5,661    3      +0.5%   \n",
      "Q1         ‚Ç¨   5,631    4      Base    \n",
      "\n",
      "Quarterly Summary:\n",
      "--------------------\n",
      "Best quarter: Q4 (‚Ç¨6,125)\n",
      "Worst quarter: Q1 (‚Ç¨5,631)\n",
      "Performance gap: 8.5%\n",
      "\n",
      "Quarter-to-Quarter Growth:\n",
      "-------------------------\n",
      "Q1 to Q2: +0.5%\n",
      "Q2 to Q3: +1.8%\n",
      "Q3 to Q4: +6.3%\n",
      "\n",
      "Key Insight: Q4 generates 26.4% of annual revenue\n"
     ]
    }
   ],
   "source": [
    "# Simple and robust quarterly analysis\n",
    "quarter_avg = df_viz_feat.groupby('quarter')['sales'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Quarterly Sales Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Quarter':<10} {'Avg Sales':<12} {'Rank':<6} {'vs Q1':<8}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, (quarter, sales) in enumerate(quarter_avg.items(), 1):\n",
    "    vs_q1 = ((sales - quarter_avg[1]) / quarter_avg[1]) * 100\n",
    "    vs_q1_str = f\"{vs_q1:+.1f}%\" if quarter != 1 else \"Base\"\n",
    "    print(f\"Q{quarter:<9} ‚Ç¨{sales:>8,.0f}    {i:<6} {vs_q1_str:<8}\")\n",
    "\n",
    "print(f\"\\nQuarterly Summary:\")\n",
    "print(\"-\" * 20)\n",
    "best_q = quarter_avg.index[0]\n",
    "worst_q = quarter_avg.index[-1]\n",
    "range_pct = ((quarter_avg.max() - quarter_avg.min()) / quarter_avg.mean()) * 100\n",
    "\n",
    "print(f\"Best quarter: Q{best_q} (‚Ç¨{quarter_avg[best_q]:,.0f})\")\n",
    "print(f\"Worst quarter: Q{worst_q} (‚Ç¨{quarter_avg[worst_q]:,.0f})\")\n",
    "print(f\"Performance gap: {range_pct:.1f}%\")\n",
    "\n",
    "# Growth pattern\n",
    "print(f\"\\nQuarter-to-Quarter Growth:\")\n",
    "print(\"-\" * 25)\n",
    "for q in [2, 3, 4]:\n",
    "    growth = ((quarter_avg[q] - quarter_avg[q-1]) / quarter_avg[q-1]) * 100\n",
    "    print(f\"Q{q-1} to Q{q}: {growth:+.1f}%\")\n",
    "\n",
    "print(f\"\\nKey Insight: Q{best_q} generates {((quarter_avg[best_q]/quarter_avg.sum())*100):.1f}% of annual revenue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e726ba-7b9f-4f27-8270-666a57430978",
   "metadata": {},
   "source": [
    "# ‚ú® Pro Tips : Reusable Functions - Best Practices for Reusable Functions in Data Analysis\n",
    "-----------\n",
    "Reusable functions streamline analytical workflows by promoting consistency, reducing redundancy, and improving maintainability. To maximize their effectiveness:\n",
    "\n",
    " - Design for flexibility: Use parameters and config objects to adapt logic across datasets and use cases.\n",
    "\n",
    " - Keep functions atomic: Focus each function on a single task‚Äîcleaning, aggregating, visualizing, or exporting.\n",
    "\n",
    " - Avoid side effects: Return outputs explicitly; defer file I/O or plotting to higher-level orchestration.\n",
    "\n",
    " - Document clearly: Use concise docstrings and intuitive naming for better readability and future reuse.\n",
    "\n",
    " - Centralize configuration: Store defaults and settings in external files or global dictionaries for easy updates.\n",
    "\n",
    " - Efficient function design leads to cleaner notebooks, faster iteration, and scalable analysis pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574dab5-f9a9-4670-b2a3-754052bc1fe8",
   "metadata": {},
   "source": [
    "# üîß Generalized Reusable Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249d418e-05aa-41df-96ad-34f5f45330d0",
   "metadata": {},
   "source": [
    "## Impact promo analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "997a27e3-64bd-44d3-b440-09954ede4710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ PROMOTIONAL IMPACT ANALYSIS REPORT\n",
      "============================================================\n",
      "üìä Data Overview: 814,150 records after removing closed days\n",
      "üè™ Analyzing top 10 stores: [817, 262, 1114, 251, 842, 513, 562, 788, 383, 756]\n",
      "üìà Analysis dataset: 7,702 records\n",
      "üéØ Promotional days: 3,325 (43.2%)\n",
      "üìÖ Regular days: 4,377 (56.8%)\n",
      "\n",
      "üí∞ SALES PERFORMANCE ANALYSIS\n",
      "========================================\n",
      "üéØ Average Sales (Promotional): $20,779\n",
      "üìä Average Sales (Regular): $17,473\n",
      "‚¨ÜÔ∏è  Absolute Sales Lift: $3,306\n",
      "üìà Percentage Sales Lift: +18.92%\n",
      "\n",
      "üë• CUSTOMER TRAFFIC ANALYSIS\n",
      "========================================\n",
      "üéØ Average Customers (Promotional): 2,636\n",
      "üìä Average Customers (Regular): 2,469\n",
      "‚¨ÜÔ∏è  Customer Traffic Lift: +167\n",
      "üìà Customer Traffic Lift: +6.78%\n",
      "\n",
      "üéØ EFFICIENCY & PROFITABILITY\n",
      "========================================\n",
      "üí≥ Sales per Customer (Promotional): $7.88\n",
      "üí≥ Sales per Customer (Regular): $7.08\n",
      "üìä Spending Efficiency Gain: +11.37%\n",
      "\n",
      "üìä STATISTICAL VALIDATION\n",
      "========================================\n",
      "üßÆ T-Statistic: 39.17\n",
      "üìà P-Value: 0.000000\n",
      "‚úÖ Statistically Significant: YES (Œ±=0.05)\n",
      "\n",
      "üí° KEY BUSINESS INSIGHTS\n",
      "========================================\n",
      "üëç GOOD PERFORMANCE: Promotions show solid results\n",
      "üìã RECOMMENDATION: MAINTAIN current promotional strategy\n",
      "üí∞ PRIMARY DRIVER: Promotions increase SPENDING PER VISIT (+11.4%)\n",
      "   ‚Üí Excellent basket size improvement\n",
      "\n",
      "üèÜ TOP PERFORMING STORE: #817\n",
      "   üìà Sales Lift: +32.8%\n",
      "   üë• Customer Lift: +21.1%\n",
      "   üéØ Promo Rate: 44.9%\n",
      "\n",
      "üìâ LOWEST PERFORMING STORE: #262\n",
      "   üìà Sales Lift: +32.8%\n",
      "   üë• Customer Lift: +-3.4%\n",
      "   üéØ Promo Rate: 38.1%\n",
      "\n",
      "üìä CONSISTENCY ANALYSIS:\n",
      "   üéØ Average Lift Across Stores: +19.7%\n",
      "   üìè Performance Consistency: 8/10 stores within ¬±10%\n",
      "   üëç MODERATELY CONSISTENT: Most stores benefit similarly\n",
      "\n",
      "üìÖ SEASONAL INSIGHTS:\n",
      "   üèÜ Best Month for Promos: December (+27.0%)\n",
      "   üìâ Worst Month for Promos: November (+11.2%)\n",
      "\n",
      "üéâ ANALYSIS COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def clean_promo_analysis(df, sales_col='sales', customers_col='customers', \n",
    "                        store_col='store', promo_col='promo', date_col='date', top_n=10):\n",
    "    \"\"\"\n",
    "    Clean and comprehensive promotional impact analysis\n",
    "    \"\"\"\n",
    "    print(\"üéØ PROMOTIONAL IMPACT ANALYSIS REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Data preprocessing\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remove closed stores (sales = 0)\n",
    "    df_clean = df_clean[df_clean[sales_col] > 0]\n",
    "    print(f\"üìä Data Overview: {len(df_clean):,} records after removing closed days\")\n",
    "    \n",
    "    # Create binary promo flag\n",
    "    df_clean['promo_flag'] = (df_clean[promo_col] == 'Promo').astype(int)\n",
    "    \n",
    "    # Get top stores by average sales\n",
    "    top_stores = df_clean.groupby(store_col)[sales_col].mean().nlargest(top_n).index\n",
    "    df_analysis = df_clean[df_clean[store_col].isin(top_stores)]\n",
    "    \n",
    "    print(f\"üè™ Analyzing top {len(top_stores)} stores: {list(top_stores)}\")\n",
    "    print(f\"üìà Analysis dataset: {len(df_analysis):,} records\")\n",
    "    \n",
    "    # Split data\n",
    "    promo_data = df_analysis[df_analysis['promo_flag'] == 1]\n",
    "    non_promo_data = df_analysis[df_analysis['promo_flag'] == 0]\n",
    "    \n",
    "    print(f\"üéØ Promotional days: {len(promo_data):,} ({len(promo_data)/len(df_analysis)*100:.1f}%)\")\n",
    "    print(f\"üìÖ Regular days: {len(non_promo_data):,} ({len(non_promo_data)/len(df_analysis)*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    results = {}\n",
    "    \n",
    "    # Sales metrics\n",
    "    promo_avg_sales = promo_data[sales_col].mean()\n",
    "    non_promo_avg_sales = non_promo_data[sales_col].mean()\n",
    "    sales_lift = promo_avg_sales - non_promo_avg_sales\n",
    "    sales_lift_pct = (sales_lift / non_promo_avg_sales) * 100\n",
    "    \n",
    "    # Customer metrics  \n",
    "    promo_avg_customers = promo_data[customers_col].mean()\n",
    "    non_promo_avg_customers = non_promo_data[customers_col].mean()\n",
    "    customer_lift = promo_avg_customers - non_promo_avg_customers\n",
    "    customer_lift_pct = (customer_lift / non_promo_avg_customers) * 100\n",
    "    \n",
    "    # Efficiency metrics\n",
    "    promo_sales_per_customer = promo_avg_sales / promo_avg_customers\n",
    "    non_promo_sales_per_customer = non_promo_avg_sales / non_promo_avg_customers\n",
    "    efficiency_improvement = ((promo_sales_per_customer - non_promo_sales_per_customer) / \n",
    "                             non_promo_sales_per_customer) * 100\n",
    "    \n",
    "    # Statistical test\n",
    "    t_stat, p_value = ttest_ind(promo_data[sales_col], non_promo_data[sales_col])\n",
    "    is_significant = p_value < 0.05\n",
    "    \n",
    "    # Store-level analysis\n",
    "    store_results = []\n",
    "    for store in top_stores:\n",
    "        store_data = df_analysis[df_analysis[store_col] == store]\n",
    "        store_promo = store_data[store_data['promo_flag'] == 1]\n",
    "        store_regular = store_data[store_data['promo_flag'] == 0]\n",
    "        \n",
    "        if len(store_promo) > 0 and len(store_regular) > 0:\n",
    "            store_sales_lift = ((store_promo[sales_col].mean() - store_regular[sales_col].mean()) / \n",
    "                              store_regular[sales_col].mean()) * 100\n",
    "            store_customer_lift = ((store_promo[customers_col].mean() - store_regular[customers_col].mean()) / \n",
    "                                 store_regular[customers_col].mean()) * 100\n",
    "            \n",
    "            store_results.append({\n",
    "                'Store': store,\n",
    "                'Promo Days': len(store_promo),\n",
    "                'Regular Days': len(store_regular),\n",
    "                'Promo Rate (%)': len(store_promo) / len(store_data) * 100,\n",
    "                'Sales Lift (%)': store_sales_lift,\n",
    "                'Customer Lift (%)': store_customer_lift,\n",
    "                'Promo Avg Sales': store_promo[sales_col].mean(),\n",
    "                'Regular Avg Sales': store_regular[sales_col].mean(),\n",
    "                'Promo Avg Customers': store_promo[customers_col].mean(),\n",
    "                'Regular Avg Customers': store_regular[customers_col].mean()\n",
    "            })\n",
    "    \n",
    "    store_df = pd.DataFrame(store_results)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nüí∞ SALES PERFORMANCE ANALYSIS\")\n",
    "    print(f\"=\"*40)\n",
    "    print(f\"üéØ Average Sales (Promotional): ${promo_avg_sales:,.0f}\")\n",
    "    print(f\"üìä Average Sales (Regular): ${non_promo_avg_sales:,.0f}\")\n",
    "    print(f\"‚¨ÜÔ∏è  Absolute Sales Lift: ${sales_lift:,.0f}\")\n",
    "    print(f\"üìà Percentage Sales Lift: +{sales_lift_pct:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nüë• CUSTOMER TRAFFIC ANALYSIS\") \n",
    "    print(f\"=\"*40)\n",
    "    print(f\"üéØ Average Customers (Promotional): {promo_avg_customers:,.0f}\")\n",
    "    print(f\"üìä Average Customers (Regular): {non_promo_avg_customers:,.0f}\")\n",
    "    print(f\"‚¨ÜÔ∏è  Customer Traffic Lift: +{customer_lift:.0f}\")\n",
    "    print(f\"üìà Customer Traffic Lift: +{customer_lift_pct:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nüéØ EFFICIENCY & PROFITABILITY\")\n",
    "    print(f\"=\"*40)\n",
    "    print(f\"üí≥ Sales per Customer (Promotional): ${promo_sales_per_customer:.2f}\")\n",
    "    print(f\"üí≥ Sales per Customer (Regular): ${non_promo_sales_per_customer:.2f}\")\n",
    "    print(f\"üìä Spending Efficiency Gain: +{efficiency_improvement:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nüìä STATISTICAL VALIDATION\")\n",
    "    print(f\"=\"*40)\n",
    "    print(f\"üßÆ T-Statistic: {t_stat:.2f}\")\n",
    "    print(f\"üìà P-Value: {p_value:.6f}\")\n",
    "    print(f\"‚úÖ Statistically Significant: {'YES' if is_significant else 'NO'} (Œ±=0.05)\")\n",
    "    \n",
    "    # Business insights\n",
    "    print(f\"\\nüí° KEY BUSINESS INSIGHTS\")\n",
    "    print(f\"=\"*40)\n",
    "    \n",
    "    if sales_lift_pct > 50:\n",
    "        print(f\"üöÄ EXCEPTIONAL PERFORMANCE: Promotions drive outstanding sales growth!\")\n",
    "        recommendation = \"MAXIMIZE promotional frequency - ROI is excellent\"\n",
    "    elif sales_lift_pct > 25:\n",
    "        print(f\"‚úÖ STRONG PERFORMANCE: Promotions are highly effective\")\n",
    "        recommendation = \"INCREASE promotional activities strategically\"\n",
    "    elif sales_lift_pct > 10:\n",
    "        print(f\"üëç GOOD PERFORMANCE: Promotions show solid results\")\n",
    "        recommendation = \"MAINTAIN current promotional strategy\"\n",
    "    elif sales_lift_pct > 0:\n",
    "        print(f\"‚ö†Ô∏è  WEAK PERFORMANCE: Minimal promotional benefit\")\n",
    "        recommendation = \"REVIEW promotional costs vs benefits\"\n",
    "    else:\n",
    "        print(f\"‚ùå NEGATIVE IMPACT: Promotions may be hurting performance\")\n",
    "        recommendation = \"URGENT REVIEW of promotional strategy needed\"\n",
    "    \n",
    "    print(f\"üìã RECOMMENDATION: {recommendation}\")\n",
    "    \n",
    "    # Traffic vs Spending analysis\n",
    "    if customer_lift_pct > efficiency_improvement:\n",
    "        print(f\"üë• PRIMARY DRIVER: Promotions mainly drive FOOT TRAFFIC (+{customer_lift_pct:.1f}%)\")\n",
    "        print(f\"   ‚Üí Focus on conversion and upselling during promotions\")\n",
    "    elif efficiency_improvement > customer_lift_pct:\n",
    "        print(f\"üí∞ PRIMARY DRIVER: Promotions increase SPENDING PER VISIT (+{efficiency_improvement:.1f}%)\")\n",
    "        print(f\"   ‚Üí Excellent basket size improvement\")\n",
    "    else:\n",
    "        print(f\"‚öñÔ∏è  BALANCED IMPACT: Both traffic and spending improve equally\")\n",
    "    \n",
    "    # Store performance insights\n",
    "    if not store_df.empty:\n",
    "        best_store = store_df.loc[store_df['Sales Lift (%)'].idxmax()]\n",
    "        worst_store = store_df.loc[store_df['Sales Lift (%)'].idxmin()]\n",
    "        \n",
    "        print(f\"\\nüèÜ TOP PERFORMING STORE: #{int(best_store['Store'])}\")\n",
    "        print(f\"   üìà Sales Lift: +{best_store['Sales Lift (%)']:.1f}%\")\n",
    "        print(f\"   üë• Customer Lift: +{best_store['Customer Lift (%)']:.1f}%\")\n",
    "        print(f\"   üéØ Promo Rate: {best_store['Promo Rate (%)']:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nüìâ LOWEST PERFORMING STORE: #{int(worst_store['Store'])}\")\n",
    "        print(f\"   üìà Sales Lift: +{best_store['Sales Lift (%)']:.1f}%\")\n",
    "        print(f\"   üë• Customer Lift: +{worst_store['Customer Lift (%)']:.1f}%\")\n",
    "        print(f\"   üéØ Promo Rate: {worst_store['Promo Rate (%)']:.1f}%\")\n",
    "        \n",
    "        avg_lift = store_df['Sales Lift (%)'].mean()\n",
    "        consistent_stores = ((store_df['Sales Lift (%)'] - avg_lift).abs() < 10).sum()\n",
    "        \n",
    "        print(f\"\\nüìä CONSISTENCY ANALYSIS:\")\n",
    "        print(f\"   üéØ Average Lift Across Stores: +{avg_lift:.1f}%\")\n",
    "        print(f\"   üìè Performance Consistency: {consistent_stores}/{len(store_df)} stores within ¬±10%\")\n",
    "        \n",
    "        if consistent_stores / len(store_df) > 0.8:\n",
    "            print(f\"   ‚úÖ HIGHLY CONSISTENT: Promotions work well across all stores\")\n",
    "        elif consistent_stores / len(store_df) > 0.6:\n",
    "            print(f\"   üëç MODERATELY CONSISTENT: Most stores benefit similarly\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  INCONSISTENT: Results vary significantly by store\")\n",
    "            print(f\"   ‚Üí Investigate store-specific factors affecting promotional performance\")\n",
    "    \n",
    "    # Time-based insights (if date available)\n",
    "    if date_col in df_analysis.columns:\n",
    "        df_analysis['month'] = pd.to_datetime(df_analysis[date_col]).dt.month_name()\n",
    "        df_analysis['weekday'] = pd.to_datetime(df_analysis[date_col]).dt.day_name()\n",
    "        \n",
    "        # Monthly performance\n",
    "        monthly_promo = df_analysis[df_analysis['promo_flag']==1].groupby('month')[sales_col].mean()\n",
    "        monthly_regular = df_analysis[df_analysis['promo_flag']==0].groupby('month')[sales_col].mean()\n",
    "        monthly_lift = ((monthly_promo - monthly_regular) / monthly_regular * 100).round(1)\n",
    "        \n",
    "        best_month = monthly_lift.idxmax()\n",
    "        worst_month = monthly_lift.idxmin()\n",
    "        \n",
    "        print(f\"\\nüìÖ SEASONAL INSIGHTS:\")\n",
    "        print(f\"   üèÜ Best Month for Promos: {best_month} (+{monthly_lift[best_month]:.1f}%)\")\n",
    "        print(f\"   üìâ Worst Month for Promos: {worst_month} (+{monthly_lift[worst_month]:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüéâ ANALYSIS COMPLETE!\")\n",
    "    \n",
    "    # Return structured results\n",
    "    return {\n",
    "        'summary_metrics': {\n",
    "            'sales_lift_pct': sales_lift_pct,\n",
    "            'customer_lift_pct': customer_lift_pct,\n",
    "            'efficiency_improvement': efficiency_improvement,\n",
    "            'statistical_significance': is_significant,\n",
    "            'p_value': p_value\n",
    "        },\n",
    "        'store_performance': store_df,\n",
    "        'raw_data': {\n",
    "            'promo_avg_sales': promo_avg_sales,\n",
    "            'regular_avg_sales': non_promo_avg_sales,\n",
    "            'promo_avg_customers': promo_avg_customers,\n",
    "            'regular_avg_customers': non_promo_avg_customers\n",
    "        }\n",
    "    }\n",
    "\n",
    "def create_promo_visualization(results_dict, store_df):\n",
    "    \"\"\"\n",
    "    Create visualizations for promotional analysis\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Sales comparison\n",
    "    metrics = ['Promo', 'Regular']\n",
    "    sales_values = [results_dict['raw_data']['promo_avg_sales'], \n",
    "                   results_dict['raw_data']['regular_avg_sales']]\n",
    "    \n",
    "    bars1 = ax1.bar(metrics, sales_values, color=['#ff6b6b', '#4ecdc4'], alpha=0.8)\n",
    "    ax1.set_title('Average Sales: Promotional vs Regular Days', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Average Sales ($)')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'${height:,.0f}', ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    # 2. Customer traffic comparison\n",
    "    customer_values = [results_dict['raw_data']['promo_avg_customers'],\n",
    "                      results_dict['raw_data']['regular_avg_customers']]\n",
    "    \n",
    "    bars2 = ax2.bar(metrics, customer_values, color=['#ff9f43', '#54a0ff'], alpha=0.8)\n",
    "    ax2.set_title('Average Customer Traffic: Promotional vs Regular Days', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Average Customers')\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:,.0f}', ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    # 3. Store performance distribution\n",
    "    if not store_df.empty:\n",
    "        ax3.hist(store_df['Sales Lift (%)'], bins=8, alpha=0.7, color='#ff6b6b', edgecolor='black')\n",
    "        ax3.set_title('Distribution of Sales Lift Across Stores', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Sales Lift (%)')\n",
    "        ax3.set_ylabel('Number of Stores')\n",
    "        ax3.axvline(store_df['Sales Lift (%)'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {store_df[\"Sales Lift (%)\"].mean():.1f}%')\n",
    "        ax3.legend()\n",
    "    \n",
    "    # 4. Key metrics summary\n",
    "    lift_pct = results_dict['summary_metrics']['sales_lift_pct']\n",
    "    customer_lift_pct = results_dict['summary_metrics']['customer_lift_pct']\n",
    "    efficiency = results_dict['summary_metrics']['efficiency_improvement']\n",
    "    \n",
    "    metrics_names = ['Sales Lift', 'Customer Lift', 'Efficiency Gain']\n",
    "    metrics_values = [lift_pct, customer_lift_pct, efficiency]\n",
    "    colors = ['#ff6b6b', '#4ecdc4', '#45aaf2']\n",
    "    \n",
    "    bars4 = ax4.bar(metrics_names, metrics_values, color=colors, alpha=0.8)\n",
    "    ax4.set_title('Key Performance Metrics (%)', fontsize=14, fontweight='bold')\n",
    "    ax4.set_ylabel('Improvement (%)')\n",
    "    \n",
    "    for bar in bars4:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'+{height:.1f}%', ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "results = clean_promo_analysis(df_viz_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "571b6b1d-ba96-4299-a935-6d4a8bcfcfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_df' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# To pull train_df from one notebook to another in JupyterLab\n",
    "%store train_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b33402-11dd-471a-b9c2-effe7f65283b",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4734fdac-47a4-4b96-9633-c77a9fd333be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Ingestion and Exploratory Data Analysis completed successfully!\n",
      "üóìÔ∏è Analysis Date: \u001b[1m2025-08-13 21:20:08\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Data Ingestion and Exploratory Data Analysis completed successfully!\")\n",
    "print(f\"üóìÔ∏è Analysis Date: {bold_start}{pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}{bold_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb3d94-675d-4cea-bc83-2e995e1e8c94",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a578a31-6bf1-4605-8d14-c69c3df0809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Analysis Summary\n",
      "üü¢ Begin Date: \u001b[1m2025-08-13 21:19:58\u001b[0m\n",
      "‚úÖ End Date:   \u001b[1m2025-08-13 21:20:08\u001b[0m\n",
      "‚è±Ô∏è Duration:   \u001b[1m0 days 00:00:10.512181\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# End analysis\n",
    "analysis_end = pd.Timestamp.now()\n",
    "duration = analysis_end - analysis_begin\n",
    "\n",
    "# Final summary print\n",
    "print(\"\\nüìã Analysis Summary\")\n",
    "print(f\"üü¢ Begin Date: {bold_start}{analysis_begin.strftime('%Y-%m-%d %H:%M:%S')}{bold_end}\")\n",
    "print(f\"‚úÖ End Date:   {bold_start}{analysis_end.strftime('%Y-%m-%d %H:%M:%S')}{bold_end}\")\n",
    "print(f\"‚è±Ô∏è Duration:   {bold_start}{str(duration)}{bold_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045b4131-f856-44b5-81e8-1f72274952fa",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "## Project Design Rationale: Notebook Separation\n",
    "\n",
    "To ensure **clarity, maintainability, and scalability** while adhering to **GitHub's file size limitations**, each **.ipynb notebook** should be modularized by task‚Äîallowing for **streamlined version control, easier collaboration, and more efficient** long-term project management."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
